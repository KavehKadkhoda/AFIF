{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFIF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KavehKadkhoda/AFIF/blob/main/AFIF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Y9j8FGp5_j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d98f5bc5-089a-49a9-c803-16a218a7df16"
      },
      "source": [
        "\"\"\"\n",
        "Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser.\n",
        "https://colab.research.google.com\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nColaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser.\\nhttps://colab.research.google.com\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "hs0KQpF7O6Pe",
        "outputId": "802a9283-887c-4279-9c27-b33b7c99bbf6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "DATA: Add shortcut to your google drive\n",
        "\n",
        "X___DBLP___Infomap___X.csv :\n",
        "https://drive.google.com/file/d/1AmCl-fo_BnijCzA6mn7ptOpAw2jfuPRF/view?usp=sharing\n",
        "\n",
        "y___DBLP___Infomap___y.csv :\n",
        "https://drive.google.com/file/d/1F94QFZupoNiR8uUe6-uz-8vt3Dg4-ymQ/view?usp=sharing\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nAdd shortcut to your google drive\\n\\nX___DBLP___Infomap___X.csv :\\nhttps://drive.google.com/file/d/1AmCl-fo_BnijCzA6mn7ptOpAw2jfuPRF/view?usp=sharing\\n\\ny___DBLP___Infomap___y.csv :\\nhttps://drive.google.com/file/d/1F94QFZupoNiR8uUe6-uz-8vt3Dg4-ymQ/view?usp=sharing\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7blLK7FxSaP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dfdf6b7-edbe-47cf-b73c-b6e95bb18e60"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Apr  4 17:20:49 2021\n",
        "\n",
        "@author: Kaveh Kadkhoda\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Data: split to train, val, test\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" reading DATA \"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "df_X = pd.read_csv('drive/MyDrive/X___DBLP___Infomap___X.csv')\n",
        "df_y = pd.read_csv('drive/MyDrive/y___DBLP___Infomap___y.csv')\n",
        "\n",
        "\n",
        "\n",
        "#all\n",
        "\n",
        "X = df_X[['size_t-2', 'evolution_t-2', 'constraint_t-2', 'core_number_t-2', 'number_of_cliques_t-2', 'density_t-2', 'algebraic_connectivity_t-2', 'wiener_index_t-2', 'effective_size_t-2', 'global_efficiency_t-2', 'local_efficiency_t-2', 'average_clustering_t-2', 'transitivity_t-2', 'harmonic_centrality_t-2', 'estrada_index_t-2', 'betweenness_t-2', 'load_centrality_t-2', 'edge_betweenness_t-2', 'closeness_t-2', 'degree_assortativity_t-2', 'square_clustering_t-2', 'average_neighbor_degree_t-2', 'pagerank_t-2', 'katz_t-2', 'clique_number_t-2', 'node_connectivity_t-2', 'second_order_t-2', 'diameter_t-2', 'edge_t-2',\n",
        "              'size_t-1', 'evolution_t-1', 'constraint_t-1', 'core_number_t-1', 'number_of_cliques_t-1', 'density_t-1', 'algebraic_connectivity_t-1', 'wiener_index_t-1', 'effective_size_t-1', 'global_efficiency_t-1', 'local_efficiency_t-1', 'average_clustering_t-1', 'transitivity_t-1', 'harmonic_centrality_t-1', 'estrada_index_t-1', 'betweenness_t-1', 'load_centrality_t-1', 'edge_betweenness_t-1', 'closeness_t-1', 'degree_assortativity_t-1', 'square_clustering_t-1', 'average_neighbor_degree_t-1', 'pagerank_t-1', 'katz_t-1', 'clique_number_t-1', 'node_connectivity_t-1', 'second_order_t-1', 'diameter_t-1', 'edge_t-1',\n",
        "              'size_t', 'evolution_t', 'constraint_t', 'core_number_t', 'number_of_cliques_t', 'density_t', 'algebraic_connectivity_t', 'wiener_index_t', 'effective_size_t', 'global_efficiency_t', 'local_efficiency_t', 'average_clustering_t', 'transitivity_t', 'harmonic_centrality_t', 'estrada_index_t', 'betweenness_t', 'load_centrality_t', 'edge_betweenness_t', 'closeness_t', 'degree_assortativity_t', 'square_clustering_t', 'average_neighbor_degree_t', 'pagerank_t', 'katz_t', 'clique_number_t', 'node_connectivity_t', 'second_order_t', 'diameter_t', 'edge_t']]\n",
        "\n",
        "y = df_y['evolution_next']\n",
        "\n",
        "\n",
        "# for LIGHTgbm\n",
        "y.replace(2,1, inplace=True)\n",
        "y.replace(3,2, inplace=True)\n",
        "y.replace(4,3, inplace=True)\n",
        "y.replace(5,4, inplace=True)\n",
        "y.replace(6,5, inplace=True)\n",
        "y.replace(7,6, inplace=True)\n",
        "y.replace(8,7, inplace=True)\n",
        "y.replace(9,8, inplace=True)\n",
        "y.replace(10,9, inplace=True)\n",
        "y.replace(11,10, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into validation set\n",
        "X_remained, X_test, y_remained, y_test = train_test_split(X, y, test_size = 0.2, random_state = 52, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_remained, y_remained, test_size = 0.25, random_state = 52, stratify=y_remained)\n",
        "\n",
        "\n",
        "X_train.round(5)\n",
        "X_val.round(5)\n",
        "X_test.round(5)\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "start111 = time.time()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Heap part\n",
        "We used the priorityq (An object-oriented priority queue with updatable priorities).\n",
        "https://github.com/elplatt/python-priorityq\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import heapq as hq\n",
        "\n",
        "__all__ = ['MappedQueue']\n",
        "\n",
        "\n",
        "class MappedQueue(object):\n",
        "    def __init__(self, data=[]):\n",
        "        \"\"\"Priority queue class with updatable priorities.\n",
        "        \"\"\"\n",
        "        self.h = list(data)\n",
        "        self.d = dict()\n",
        "        self._heapify()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.h)\n",
        "\n",
        "    def _heapify(self):\n",
        "        \"\"\"Restore heap invariant and recalculate map.\"\"\"\n",
        "        hq.heapify(self.h)\n",
        "        self.d = dict([(elt, pos) for pos, elt in enumerate(self.h)])\n",
        "        if len(self.h) != len(self.d):\n",
        "            raise AssertionError(\"Heap contains duplicate elements\")\n",
        "\n",
        "    def push(self, elt):\n",
        "        \"\"\"Add an element to the queue.\"\"\"\n",
        "        # If element is already in queue, do nothing\n",
        "        if elt in self.d:\n",
        "            return False\n",
        "        # Add element to heap and dict\n",
        "        pos = len(self.h)\n",
        "        self.h.append(elt)\n",
        "        self.d[elt] = pos\n",
        "        # Restore invariant by sifting down\n",
        "        self._siftdown(pos)\n",
        "        return True\n",
        "\n",
        "    def pop(self):\n",
        "        \"\"\"Remove and return the smallest element in the queue.\"\"\"\n",
        "        # Remove smallest element\n",
        "        elt = self.h[0]\n",
        "        del self.d[elt]\n",
        "        # If elt is last item, remove and return\n",
        "        if len(self.h) == 1:\n",
        "            self.h.pop()\n",
        "            return elt\n",
        "        # Replace root with last element\n",
        "        last = self.h.pop()\n",
        "        self.h[0] = last\n",
        "        self.d[last] = 0\n",
        "        # Restore invariant by sifting up, then down\n",
        "        pos = self._siftup(0)\n",
        "        self._siftdown(pos)\n",
        "        # Return smallest element\n",
        "        return elt\n",
        "\n",
        "    def update(self, elt, new):\n",
        "        \"\"\"Replace an element in the queue with a new one.\"\"\"\n",
        "        # Replace\n",
        "        pos = self.d[elt]\n",
        "        self.h[pos] = new\n",
        "        del self.d[elt]\n",
        "        self.d[new] = pos\n",
        "        # Restore invariant by sifting up, then down\n",
        "        pos = self._siftup(pos)\n",
        "        self._siftdown(pos)\n",
        "\n",
        "    def remove(self, elt):\n",
        "        \"\"\"Remove an element from the queue.\"\"\"\n",
        "        # Find and remove element\n",
        "        try:\n",
        "            result = [element for element in self.d if element[1] == elt]\n",
        "            pos = self.d[result[0]]\n",
        "            del self.d[result[0]]\n",
        "        except Exception:\n",
        "            # Not in queue\n",
        "            return\n",
        "        # If elt is last item, remove and return\n",
        "        if pos == len(self.h) - 1:\n",
        "            self.h.pop()\n",
        "            return\n",
        "        # Replace elt with last element\n",
        "        last = self.h.pop()\n",
        "        self.h[pos] = last\n",
        "        self.d[last] = pos\n",
        "        # Restore invariant by sifting up, then down\n",
        "        pos = self._siftup(pos)\n",
        "        self._siftdown(pos)\n",
        "\n",
        "    def _siftup(self, pos):\n",
        "        \"\"\"Move element at pos down to a leaf by repeatedly moving the smaller\n",
        "        child up.\"\"\"\n",
        "        h, d = self.h, self.d\n",
        "        elt = h[pos]\n",
        "        # Continue until element is in a leaf\n",
        "        end_pos = len(h)\n",
        "        left_pos = (pos << 1) + 1\n",
        "        while left_pos < end_pos:\n",
        "            # Left child is guaranteed to exist by loop predicate\n",
        "            left = h[left_pos]\n",
        "            try:\n",
        "                right_pos = left_pos + 1\n",
        "                right = h[right_pos]\n",
        "                # Out-of-place, swap with left unless right is smaller\n",
        "                if right < left:\n",
        "                    h[pos], h[right_pos] = right, elt\n",
        "                    pos, right_pos = right_pos, pos\n",
        "                    d[elt], d[right] = pos, right_pos\n",
        "                else:\n",
        "                    h[pos], h[left_pos] = left, elt\n",
        "                    pos, left_pos = left_pos, pos\n",
        "                    d[elt], d[left] = pos, left_pos\n",
        "            except IndexError:\n",
        "                # Left leaf is the end of the heap, swap\n",
        "                h[pos], h[left_pos] = left, elt\n",
        "                pos, left_pos = left_pos, pos\n",
        "                d[elt], d[left] = pos, left_pos\n",
        "            # Update left_pos\n",
        "            left_pos = (pos << 1) + 1\n",
        "        return pos\n",
        "\n",
        "    def _siftdown(self, pos):\n",
        "        \"\"\"Restore invariant by repeatedly replacing out-of-place element with\n",
        "        its parent.\"\"\"\n",
        "        h, d = self.h, self.d\n",
        "        elt = h[pos]\n",
        "        # Continue until element is at root\n",
        "        while pos > 0:\n",
        "            parent_pos = (pos - 1) >> 1\n",
        "            parent = h[parent_pos]\n",
        "            if parent > elt:\n",
        "                # Swap out-of-place element with parent\n",
        "                h[parent_pos], h[pos] = elt, parent\n",
        "                parent_pos, pos = pos, parent_pos\n",
        "                d[elt] = pos\n",
        "                d[parent] = parent_pos\n",
        "            else:\n",
        "                # Invariant is satisfied\n",
        "                break\n",
        "        return pos\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "lgbm: trained with data from time window t.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "properties = ['size_t', 'evolution_t', 'constraint_t', 'core_number_t', 'number_of_cliques_t', 'density_t', 'algebraic_connectivity_t',\n",
        "              'wiener_index_t', 'effective_size_t', 'global_efficiency_t', 'local_efficiency_t', 'average_clustering_t', 'transitivity_t',\n",
        "              'harmonic_centrality_t', 'estrada_index_t', 'betweenness_t', 'load_centrality_t', 'edge_betweenness_t', 'closeness_t',\n",
        "              'degree_assortativity_t', 'square_clustering_t', 'average_neighbor_degree_t', 'pagerank_t', 'katz_t', 'clique_number_t',\n",
        "              'node_connectivity_t', 'second_order_t', 'diameter_t', 'edge_t']\n",
        "\n",
        "x = X_train[properties].copy()\n",
        "y = y_train.copy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calc_lighGBM(x, y):\n",
        "\n",
        "    \n",
        "\n",
        "    model = lgb.LGBMClassifier(objective=\"multiclass\", random_state=10, boosting='gbdt')\n",
        "    model.fit(x, y)\n",
        "\n",
        "    model.booster_.feature_importance(importance_type='split')\n",
        "    fea_imp_ = pd.DataFrame({'cols': x.columns, 'fea_imp': model.feature_importances_})\n",
        "    fea_imp_.loc[fea_imp_.fea_imp > 0].sort_values(by=['fea_imp'], ascending=False)\n",
        "\n",
        "\n",
        "    d = dict(zip(fea_imp_.cols, fea_imp_.fea_imp))\n",
        "\n",
        "    list_lgbm = [(-value, key) for key,value in d.items()]\n",
        "    return list_lgbm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "list___lgbm = calc_lighGBM(x, y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "correlation: trained with data from time window t.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "properties = ['size_t', 'evolution_t', 'constraint_t', 'core_number_t', 'number_of_cliques_t', 'density_t', 'algebraic_connectivity_t',\n",
        "              'wiener_index_t', 'effective_size_t', 'global_efficiency_t', 'local_efficiency_t', 'average_clustering_t', 'transitivity_t',\n",
        "              'harmonic_centrality_t', 'estrada_index_t', 'betweenness_t', 'load_centrality_t', 'edge_betweenness_t', 'closeness_t',\n",
        "              'degree_assortativity_t', 'square_clustering_t', 'average_neighbor_degree_t', 'pagerank_t', 'katz_t', 'clique_number_t',\n",
        "              'node_connectivity_t', 'second_order_t', 'diameter_t', 'edge_t']\n",
        "\n",
        "x = X_train[properties].copy()\n",
        "\n",
        "\n",
        "def calc_property_correlation(x):\n",
        "    x = x.replace([np.inf, -np.inf], np.nan)\n",
        "    x.fillna(0, inplace=True)\n",
        "\n",
        "    X = x.copy()\n",
        "    c = X.columns\n",
        "\n",
        "    min_res = []\n",
        "    max_res = []\n",
        "    li_dict = {}\n",
        "    for i in range(29):\n",
        "        evosum = 0\n",
        "        for j in range(29):\n",
        "            a = X[c[i]]\n",
        "            if i != j:\n",
        "                b = X[c[j]]\n",
        "                k = stats.spearmanr(a, b)\n",
        "                kvv2 = abs(k[0])\n",
        "                evosum = evosum + kvv2\n",
        "        min_res.append((evosum,properties[i]))\n",
        "        max_res.append((-1*evosum,properties[i]))\n",
        "        li_dict[properties[i]] = evosum\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return li_dict, min_res, max_res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "li_dict, list_corr_min, list_corr_max = calc_property_correlation(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "random forest: Finding by train and val data from time window t, t-1, t-2\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "def finding_subalgorithm(heap_list, X_train, X_val, y_train, y_val):\n",
        "\n",
        "    model = RandomForestClassifier(random_state=39, n_jobs=-1, n_estimators=10)\n",
        "\n",
        "    candidate_features = []\n",
        "    selected_features = []\n",
        "    s_f = []\n",
        "    balance_accuracy_base = 0\n",
        "    balance_accuracy = 0\n",
        "    potential_candidate = None\n",
        "    while True:\n",
        "        for heap in heap_list:\n",
        "            feature = heap.pop()\n",
        "            candidate_features.append(feature[1])\n",
        "            for h in heap_list:\n",
        "                h.remove(feature[1])\n",
        "            \n",
        "        improvement = 0\n",
        "        candidate_features_copy = candidate_features.copy()\n",
        "        for feature in candidate_features:\n",
        "            s_f_test = s_f.copy()\n",
        "            s_f_test.append(feature)\n",
        "            s_f_test.append(feature + '-1')\n",
        "            s_f_test.append(feature + '-2')\n",
        "\n",
        "            X_train_rf = X_train[s_f_test].copy()\n",
        "            X_train_rf = X_train_rf.fillna(0)\n",
        "\n",
        "            X_val_rf = X_val[s_f_test].copy()\n",
        "            X_val_rf = X_val_rf.fillna(0)\n",
        "\n",
        "            model.fit(X_train_rf, y_train)\n",
        "\n",
        "            # Make validation predictions\n",
        "            test_preds = model.predict_proba(X_val_rf)\n",
        "            preds_df = pd.DataFrame(test_preds, columns=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "            # Convert into predictions\n",
        "            preds_df['prediction'] = preds_df[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]].idxmax(axis=1)\n",
        "            preds_df['confidence'] = preds_df[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]].max(axis=1)\n",
        "            preds_df.head()\n",
        "\n",
        "            balance_accuracy_feature = round(balanced_accuracy_score(y_val, preds_df['prediction']), 3)\n",
        "            improvement_feature = balance_accuracy_feature - balance_accuracy_base\n",
        "            if improvement_feature > improvement:\n",
        "                improvement = improvement_feature\n",
        "                potential_candidate = feature\n",
        "                balance_accuracy = balance_accuracy_feature\n",
        "            if improvement_feature <= 0:\n",
        "                candidate_features_copy.remove(feature)\n",
        "        candidate_features = candidate_features_copy.copy()\n",
        "        if improvement > 0:\n",
        "            s_f.append(potential_candidate)\n",
        "            s_f.append(potential_candidate + '-1')\n",
        "            s_f.append(potential_candidate + '-2')\n",
        "            selected_features.append(potential_candidate)\n",
        "            candidate_features.remove(potential_candidate)\n",
        "            balance_accuracy_base = balance_accuracy\n",
        "        else:\n",
        "            return selected_features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "heap_corr_min = MappedQueue(list_corr_min)\n",
        "heap_corr_max = MappedQueue(list_corr_max)\n",
        "heap___lgbm = MappedQueue(list___lgbm)\n",
        "heap_list = [heap_corr_max, heap_corr_min, heap___lgbm]\n",
        "\n",
        "\n",
        "result = finding_subalgorithm(heap_list, X_train, X_val, y_train, y_val)\n",
        "\n",
        "\n",
        "\n",
        "end111 = time.time()\n",
        "\n",
        "print(\"--------------------\")\n",
        "print(\"afif run time:\")\n",
        "print(end111 - start111)\n",
        "print(\"--------------------\")\n",
        "\n",
        "print('**********************************')\n",
        "print('The most prominent features are:')\n",
        "for i in result:\n",
        "  print(i[0:-2])\n",
        "print('**********************************')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "afif run time:\n",
            "2369.619050502777\n",
            "--------------------\n",
            "**********************************\n",
            "The most prominent features are:\n",
            "second_order\n",
            "evolution\n",
            "wiener_index\n",
            "**********************************\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}